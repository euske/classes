<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="./common.css">
<title>AI Exercises (WISE-SSS, Sep. 4, 2020)</title>
</head>
<body>

<h1>AI Exercises (WISE-SSS)</h1>
<div align=right>
Sep. 4, 2020.
</div>

<h2>Linux Fundamentals</h2>

<h3>Filesystem Basics</h3>

<p>
A filesystem is a way to <em>hierarchically</em> organize a number
of <u>files</u> into different <u>folders</u> (<u>directories</u>).
In a Linux system, every user is assigned with its own <u>home directory</u>.

<div class=figure>
<svg xmlns="http://www.w3.org/2000/svg"
     xmlns:xlink="http://www.w3.org/1999/xlink"
     version="1.1" width="500" height="300">
  <defs>
  <marker id="arrow" viewBox="-5 -5 10 10" orient="auto">
    <polygon points="-5,-5 5,0 -5,5" fill="black" stroke="none" />
  </marker>
  <symbol id="folder" width="100" height="100">
    <g stroke="black" stroke-width="2" stroke-linejoin="round" fill="#ffee88">
      <polygon points="2,2 65,2 65,80 40,80" />
      <polygon points="2,2 55,10 55,90 2,80" fill="white" />
      <path d="M10,15 L45,25 M10,25 L45,35 M10,35 L45,45 M10,45 L45,55 M10,55 L45,65 M10,65 L45,75" />
      <polygon points="2,2 40,20 40,100 2,80" />
    </g>
  </symbol>
  <symbol id="file" width="100" height="100">
    <g fill="none" stroke="black" stroke-width="2" stroke-linejoin="round">
      <polygon points="20,10 60,10 80,30 80,90 20,90" fill="white" />
      <path d="M30,25 L70,25 M30,35 L70,35 M30,45 L70,45 M30,55 L70,55 M30,65 L70,65 M30,75 L70,75" />
      <polygon points="60,10 60,30 80,30" fill="white" />
    </g>
  </symbol>
  <symbol id="disk" width="100" height="100">
    <g fill="none" stroke="black" stroke-width="2" stroke-linejoin="round">
      <polygon points="10,40 30,30 90,30 90,50 70,60 10,60" fill="white" />
      <path d="M15,42 L68,42 L68,56 M68,42 L86,33 M20,48 L30,48" />
    </g>
  </symbol>
  </defs>
  <g transform="translate(10.5,0.5) scale(0.6,0.6)">
    <use x="300" y="0" xlink:href="#disk" />
    <use x="150" y="120" xlink:href="#folder" />
    <use x="300" y="120" xlink:href="#folder" />
    <use x="410" y="120" xlink:href="#file" />
    <use x="250" y="250" xlink:href="#file" />
    <use x="340" y="250" xlink:href="#file" />
    <use x="40" y="260" xlink:href="#folder" />
    <use x="160" y="260" xlink:href="#folder" />
    <use x="210" y="390" xlink:href="#folder" />
    <use x="100" y="380" xlink:href="#file" />
    <use x="0" y="390" xlink:href="#file" />
    <g stroke="black" fill="none" stroke-width="2">
      <line x1="330" y1="70" x2="200" y2="116" />
      <line x1="340" y1="70" x2="330" y2="116" />
      <line x1="350" y1="70" x2="430" y2="116" />
      <line x1="330" y1="220" x2="300" y2="250" />
      <line x1="350" y1="220" x2="380" y2="250" />
      <line x1="170" y1="220" x2="100" y2="256" />
      <line x1="180" y1="220" x2="200" y2="256" />
      <line x1="190" y1="360" x2="160" y2="380" />
      <line x1="210" y1="360" x2="240" y2="386" />
      <line x1="65" y1="360" x2="50" y2="390" />
    </g>
    <g stroke="red" fill="none" stroke-width="4">
      <rect x="120" y="250" width="110" height="100" />
      <line x1="220" y1="350" x2="340" y2="380" />
    </g>
    <g text-anchor="end">
      <text x="300" y="50">Root</text>
      <text x="145" y="160">A</text>
      <text x="295" y="160">B</text>
      <text x="500" y="160" text-anchor="start">C</text>
      <text x="30" y="300">B</text>
      <text x="150" y="300">E</text>
      <text x="260" y="300">C</text>
      <text x="440" y="300">K</text>
      <text x="10" y="440">H</text>
      <text x="110" y="440">K</text>
      <text x="290" y="440" text-anchor="start">X</text>
      <text x="340" y="400" text-anchor="start" fill="red">Current directory</text>
    </g>
    <g style="font-family: monospace;">
      <text x="400" y="50">/</text>
    </g>
  </g>
</svg>
</div>

<div class=exercise>
<div class=header>Exercise</div>
Suppose that the current directory is indicated by the
<span style="outline:2px solid red; padding:2px;">red box</span>.
Answer the following questions:
<ol type=a>
  <li> The absolute pathname to <u>directory X</u>?
  <li> The absolute pathname to <u>file H</u>?
  <li> The absolute pathnames to <u>file C</u>?
  <li> The relative pathname to <u>directory A</u>?
  <li> The relative pathname to <u>file H</u>?
  <li> The relative pathnames to <u>file K</u>?
</ol>
</div>

<h3>Shell Basics</h3>
<p>
A shell is the main interface where you interact with
a Unix/Linux system. You type a command in, and the system
prints its result. The command line has the following syntax:
<pre>
$ <strong>command arg1 arg2 ...</strong>
</pre>
<ul>
<li> Command prompt: <code>$</code>
<li> Each argument is separated by a blank character.
<li> Use the Arrow keys to reuse/modify the previous commands.
<li> Use the <kbd>Tab</kbd> key to complete file names.
</ul>

<div class=exercise>
<div class=header>Exercise</div>
<ol type=a>
  <li> In the following command line,
    what is the command and its arguments?
<pre>$ <strong>ls -la ~/</strong></pre>
  <li> Write a command line that executes
    "<code>find</code>" with three arguments, "<code>.</code>",
    "<code>-type</code>" and "<code>f</code>".
  <li> At your home directory, type "<code>ls .</code>"
    and then press the <kbd>Tab</kbd>. What would happen?
</ol>
</div>

<h3>Linux Commands</h3>
<ul>
<li> <code>ls</code> (Show the listings)
<pre>
$ <strong>ls</strong>       <span class=comment>(Shows the current directory)</span>
$ <strong>ls ~/</strong>    <span class=comment>(Shows the home directory)</span>
$ <strong>ls ..</strong>    <span class=comment>(Shows the parent directory)</span>
$ <strong>ls <em>path</em></strong>  <span class=comment>(Shows the specific directory)</span>
</pre>
<li> <code>echo</code> (Show strings / variables)
<pre>
$ <strong>echo hello</strong>  <span class=comment>(Shows "hello")</span>
$ <strong>echo $PATH</strong>  <span class=comment>(Shows the PATH variable)</span>
</pre>
<li> <code>pwd</code> (Show the current directory)
<li> <code>cd</code> (Change the current directory)
<li> <code>mkdir</code> (Create a directory)
<li> <code>cat</code> (Display/concatenate file contents)
<li> <code>less</code> (Page file contents)
<li> <code>cp</code> (Copy files)
<li> <code>mv</code> (Move/rename files)
<li> Remote commands:
<ul>
  <li> <code>ssh-keygen</code> (Generate a public/private key pair)
  <li> <code>ssh</code> (Remote login)
  <li> <code>scp</code> (Remote file copy)
</ul>
<li> TSUBAME commands:
<ul>
  <li> <code>git</code> (Clone the code repository)
  <li> <code>qsub</code> (Submit a task)
  <li> <code>qstat</code> (Show the task status)
  <li> <code>qdel</code> (Abort a submitted task)
</ul>
<li> Other commands:
<ul>
<li> <code>mkgraph.sh</code> (Plot a graph)
</ul>
</ul>


<h2>Google Colaboratory (p.5-12)</h2>

<h3>What is it? (p.6)</h3>
<ul>
<li> Google service for machine learning / data science education.
<li> Python on cloud.
<li> Instant setup.
<li> Free for everyone.
<li> Easy to share others' code.
<li> Requires Google Account.
</ul>

<h3>How to use (p.7-12)</h3>
<ol>
<li> Login with Google Account.
<li> Log onto <a href="https://colab.research.google.com/">https://colab.research.google.com/</a>
<li> Create a new Notebook.
<li> Write a Python program and press <kbd>Run</kbd> (or <kbd>Ctrl</kbd>+<kbd>Enter</kbd>).
<li> Type in the GitHub url.
<li> Choose the ipynb file to load.
<li> Press the <kbd>Run</kbd> button in order.
</ol>


<h2>Neural Networks (p.13-39)</h2>

<h3>What can it do? (p.14-23)</h3>
<ul>
<li> Character recognition
<li> Image recognition
<li> Speech recognition
<li> Machine translation
<li> Dialogue automation
<li> Image captioning
<li> Playing video games
<li> Playing go
<li> Style translation
<li> Cartoon generation
</ul>

<h3>How it works? (p.24-33)</h3>

<ul>
<li> Linear regression (p.25):
  <strong>y</strong> =
  <em>w</em><sub>1</sub><em>x</em><sub>1</sub> +
  <em>w</em><sub>2</sub><em>x</em><sub>2</sub> +
  ... +
  <em>w</em><sub><em>n</em></sub><em>x</em><sub><em>n</em></sub> + b
  = <strong><em>w</em></strong><sup>T</sup> <strong><em>x</em></strong>
  <ul>
    <li> e.g. estimate the body fat ratio (y) based on the weight
      (<em>x</em><sub>1</sub>) and resistance (<em>x</em><sub>2</sub>).
  </ul>

<li> Model learning (p.26):
<ul>
  <li> Adjust the parameters {<em>w</em><sub>1</sub>, <em>w</em><sub>2</sub>, b}
    to minimize the estimation (&ycirc;) and the observation (y).
  <li> argmin [y - &ycirc;] =
    <em>w</em><sub>1</sub><em>x</em><sub>1</sub> +
    <em>w</em><sub>2</sub><em>x</em><sub>2</sub> + b
</ul>

<li> Model evaluation (p.27):
<ul>
<li> Test the model with evaluation data that is
  <em>independent</em> from the training data.
</ul>

<li> Extend it to Neural Network (p.28):
<ul>
  <li> Change the linear function to a <em>non-linear</em> function.
  <li> Make it a network.
</ul>

<li> Seq2seq (p.29)

<li> Learning for a Neural Network (p.30):
<ul>
  <li> Gradient Descent
  <li> Training data &rarr;
    <strong><em>x</em></strong> (input),
    <strong><em>y</em></strong> (answer), and
    <strong><em>&ycirc;</em></strong> (estimate).
    <ol>
      <li> Initialize the random <strong><em>w</em></strong>.
      <li> Compute the error.
      <li> Analyze the &Delta;<strong><em>w</em></strong> against the &Delta;error.
      <li> Update the <strong><em>w</em></strong>.
      <li> Rinse and repeat until the error is small enough.
    </ol>
</ul>

<li> Related courses (p.31):
<ul>
  <li> Fundamental:
    <ul>
      <li> Linear Algebra
      <li> Differentiation and Integration
      <li> Probability and Statistics
      <li> Numerical Analysis
    </ul>
  <li> Advanced:
    <ul>
      <li> Statistical Signal Processing
      <li> Function Analysis and Inverse Problems
      <li> Artificial Intelligence Fundamentals
      <li> Machine Learning
      <li> Logic and Inference
      <li> Brain Hyper-parallelism and Its Mathematical Fundation
      <li> Computational Linguistics
      <li> Audio Information Processing
      <li> Natural Language Processing
      <li> Medical Image Processing
    </ul>
</ul>

<li> Computational Cost of Neural Networks (p.32):
  <ul>
    <li> Consider a NN that has <em>S</em> layers.
    <li> Each layer has <em>T</em> units.
    <li> The total number of connections is <em>S</em> <em>T</em><sup>2</sup>.
    <li> To compute the value for <em>K</em> samples,
      <em>K</em> <em>S</em> <em>T</em><sup>2</sup> computation in total is needed.
      <br>
      e.g. <em>S</em>=10, <em>T</em>=4000, <em>K</em>=100000000,
      you'll get 1.6 &times; 10<sup>16</sup>.
    <li> Need to train the model for multiple times.
    <li> A <em>lot</em> of computation is needed!
  </ul>

<li> History of Computers (p.33)

</ul>

<h3>Experimentation (p.34-39)</h3>

<ul>
<li> Recipes for Neural Networks (p.34)
<ul>
<li> We prepared short recipes (scripts) for experiencing
  the training and evaluation of our NN model.
<li> Each recipe performs: data preparation,
  network training and its evaluation.
<li> In this course, we use TSUBAME Supercomputer to run the recipes.
</ul>

<li> Experiment overview (p.35)
<ul>
<li> Purpose: to understand the overall mechanism of training and testing NNs.
<li> Essential stuff:
<ul>
  <li> Training data is used for training the network.
  <li> Test data is used for evaluating the network.
  <li> To train a NN, a lot of data and computation is needed.
  <li> The meaning of evaluation metrics for each recipe:
    What is it? Is smaller/larger better?
</ul>
<li> Optional stuff:
<ul>
  <li> The network structure of each recipe.
  <li> The precise definition of each evaluation metric.
  <li> The details of the program code.
</ul>
</ul>
<li> How recipe works (p.36):
<table border>
<tr><th></th><th>Inputs</th><th>Eval. metrics</th></tr>
<tr><td>Speech</td><td>(Audio + Word labels)</td><td>Speech recognition rate</td></tr>
<tr><td>Image</td><td>(Image + Digit labels)</td><td>Image recognition rate</td></tr>
<tr><td>Translation</td><td>(Japanese text + English text)</td><td>BLEU score</td></tr>
</table>
<li> Speech Recognition Recipe (p.37):
<ul>
  <li> We construct a NN using the Speech Commands Dataset provided by Google
    as training data.
  <li> 10 Commands to be recognized:
    "yes", "no", "up", "down", "left", "right", "on", "off", "stop", "go".
</ul>
<li> Speech Recognition NN (p.38):
<ul>
<li> The system takes audio signal as input.
<li> It outputs the probability of each possible word.
<li> The word with the highest probability is chosen.
<li> Audio signal &rarr; Frequency spectrum &rarr; Feature vector.
<li> 5-layer Bi-LSTM is used.
</ul>
<li> Evaluation Metrics for Speech (p.39):
<ul>
  <li> Speech recognition rate = (Successful recognition) / (Number of trials)
</ul>
</ul>

<h3>Experimentation (p.40-46)</h3>
<ul>
<li> Login and prepare the code (p.41):
<pre>
$ <strong>ssh <u>STUDENTID</u>@login.t3.gsic.titech.ac.jp</strong>
$ <strong>git clone https://github.com/tttslab/tut-asr-voicecommand.git experiment</strong>
</pre>
<li> Run the training and evaluation (p.42):
Submit the job with <code>qsub</code> command.
(This is a trial task that will finish within about 3 minutes.)
<li> Screenshot (p.43).
<li> How to review the evaluation results (p.44):
the number can be slightly varied based on the random initialization and floating point error.
Press <kbd>q</kbd> to go back to the original screen.
<li> Trial accuracy (p.45):
  Speech recognition: 53.9%, Image recognition: 81.6%, Translation: 0.93.
  If the number is off by a few %, something must be wrong.
<li> Recipe structure (p.46):
  <ul>
    <li> <code>runtsubame.sh</code> (Prepare for running the task) on TSUBAME)
      <ul>
        <li> <code>run.sh</code> (Actual task script which does the thing)
          <ul>
            <li> <code>prep.sh</code> (Prepare the data - do nothing this time)
            <li> <code>train.sh</code> (Train the NN)
            <li> <code>eval.sh</code> (Evaluate the NN)
          </ul>
      </ul>
  </ul>
</ul>

<h3>Tweaking Parameters (p.47-64)</h3>
<p>
Now we change the training data size and see how affects
the training time or its performance.

<ul>
<li> Directory structure (p.48).
<li> Changing the data size (p.49):
  use 20% of the entire data.
<li> Jobs that take more than <u>10 minutes</u> are billed. (p.50)
<ul>
  <li> TSUBAME Points are needed!
  <li> You need to specify the billing group and maximum (potential) job duration.
  <li> Longer jobs take more points!
  <li> In this course, 6 hours at maximum is enough for each case.
</ul>
<li> How to specify the billing group. (p.51)
  Once a job is submitted, you can log out and wait.
<li> How batch job scheduling works. (p.52)
<li> Setting the data size even larger (60% - 100%). (p.53)
<li> How to examine and cancel submitted jobs. (p.54)
  You can abort a job with <code>qdel</code> command.
<li> Examining the results. (p.55-56)
<li> Plotting the accuracy. (p.57-60)
  <ul>
    <li> Run <code>mkgraph.sh</code> command to draw a graph.
    <li> The number of Xs and Ys are equal.
    <li> Arguments should be enclosed with <code>'...'</code>
    <li> For the Y values, use the actual output from the experiments.
  </ul>
<li> Copying the <code>result.png</code> onto your PC.
<pre>
$ <strong>scp STUDENTID@login.t3.gsic.titech.ac.jp:~/result.png ./</strong>
</pre>
<li> Plotting the time taken for computation. (p.63)

</ul>

<hr>
